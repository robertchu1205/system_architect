apiVersion: v1
kind: ConfigMap
metadata:
  name: tfserving-test
  labels:
    app: tfserving
data:
  model_config.txt: |
    model_config_list {
      config {
        name: 'C'
        base_path: '/models/NNs_C_0402'
        model_platform: 'tensorflow'
        model_version_policy: {
          latest: {
            num_versions: 3
          }
          # all: {}
        }
        version_labels: {
          key: 'stable'
          value: 1595841209
        }
        version_labels: {
          key: 'latest'
          value: 1595841256
        }
      }
      config {
        name: 'R'
        base_path: '/models/NNs_R_0402'
        model_platform: 'tensorflow'
        model_version_policy: {
          specific {
            versions: 1595841256
            versions: 1595841209
          }
        }
      }
    }    
  monitoring_config.txt: |
    prometheus_config {
      enable: true
      path: "/monitoring/prometheus/metrics"
    }
---
apiVersion: v1
kind: Service
metadata:
  name: tfserving-test
  labels: 
    app: tfserving
spec:
  type: NodePort
  selector:
    app: tfserving
  ports:
  - name: grpc
    port: 31100
    targetPort: grpc
    nodePort: 31100
  - name: rest
    port: 31101
    targetPort: rest
    nodePort: 31101
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tfserving-test
  labels: 
    app: tfserving
spec:
  strategy:
    # since we cannot assign gpu for tfserving, 
    # so choosing RollingUpdate to avoid crash k8s
    type: RollingUpdate
  replicas: 1
  selector:
    matchLabels:
      app: tfserving
  template:
    metadata:
      labels:
        app: tfserving
      annotations:
        configmap-sha1sum: bfa639ffad8cc76e1915b3741e8d2c61c8dee9b8
    spec:
      containers:
      - name: tfserving
        image: harbor.wzs.wistron.com.cn/tensorflow/serving:2.1.0-gpu
        args:
        - --model_config_file=/config/model_config.txt
        - --monitoring_config_file=/config/monitoring_config.txt
        - --model_config_file_poll_wait_seconds=60
        - --allow_version_labels_for_unavailable_models=true
        ports:
        - name: grpc
          containerPort: 8500
        - name: rest
          containerPort: 8501
        volumeMounts:
        - name: tfserving-test
          subPath: monitoring_config.txt
          mountPath: /config/monitoring_config.txt
        - name: data-volume
          subPath: robert/serving/20200727/model_config.txt
          mountPath: /config/model_config.txt
        - name: data-volume
          subPath: robert/serving/20200727
          mountPath: /models
        livenessProbe:
          httpGet:
            path: /monitoring/prometheus/metrics
            port: rest
        readinessProbe:
          httpGet:
            path: /monitoring/prometheus/metrics
            port: rest
        resources:
          requests:
            cpu: 1
            memory: 1Gi
          limits:
            cpu: 2
            memory: 2Gi
            nvidia.com/gpu: 1
      volumes:
      - name: tfserving-test
        configMap:
          name: tfserving-test
      - name: data-volume 
        persistentVolumeClaim: 
          claimName: dataset-local